{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1da0fd084646aeacbe140983fa9a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd231d569ed240d58e1adbf6ed7619e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515d0282161f46c8befc52f7a57ce6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/175M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320eb6b16e3548ab874bb98737ba5bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46592a1095b74121b5cce99fe1d5afb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10047 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528333acc4b84008b8b3fa7459610195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/82326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3649276a39f948d8be02f4af3aa26305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/9650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\",\"v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82326"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9650"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "total_passages = set()\n",
    "for r in dataset['test']:\n",
    "    for t in r['passages']['passage_text']:\n",
    "        total_passages.add(t)\n",
    "for r in dataset['train']:\n",
    "    for t in r['passages']['passage_text']:\n",
    "        total_passages.add(t)\n",
    "for r in dataset['validation']:\n",
    "    for t in r['passages']['passage_text']:\n",
    "        total_passages.add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767675\n"
     ]
    }
   ],
   "source": [
    "print(len(total_passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: who wrote Nothing compares to you\n",
      "\n",
      "True Passage\n",
      "Nothing Compares 2 U  is a song originally written and composed by Prince for one of his side projects, The Family. It was later made famous by Irish recording artist Sinead Sinéad'O, connor whose arrangement was released as the second single from her second studio, Album I Do Not Want What I'haven T. got Speaking about her relationship with Prince in an interview with Norwegian station NRK in November 2014 Sinead sinéad, Said i did meet him a couple of. Times we'didn t get on at. All in, fact we had a-punch.. Up she: Continued he summoned me to his house 'After Nothing compares 2.' U i made it without. him\n",
      "\n",
      "False Passage\n",
      "Answer by Mecoach50. Confidence votes 22. Nothing Compares 2 U was written by Prince and recorded by Sinead O'Connor. Yes it was written by Prince, but no it was not first recorded by Sinead O' Connor. The first to record the song was a Prince created band called The Family. The song It Had To Be You was originally written and published in 1924 by musician Isham Jones and lyricist Gus Kahn. The first recording of this song was recorded by Sam … Lanin & his Orchestra for the Okeh label.\n",
      "He has songs called I would die 4 U and If I Love U 2 Nite.. This was produced by Beresford Romeo (Jazzie B.) and Nellee Hooper, two members of the group Soul II Soul. It was Sinead Sinéad'O'connor s, Manager Fachtna'O, kelly who came up with the idea for The irish singer to cover The prince. song - Their tearful songs were their first #1 hits, but both were written by others. O'Connor's song was written by Prince, Cyrus' by a team of five professional writer/producers. Aretha Franklin covered this for her 2014 album, Aretha Franklin Sings the Great Diva Classics.\n",
      "1 In a 2006 poll for a Channel 5 program Britain's Favourite Break-up Songs, Sinead Sinéad'O'connor s version Of Nothing compares 2 u was voted. 2  FIFTH Vh1 classic Listed Sinead'O'connor s version as the second greatest classic love, song Behind Al'green  S'let S Stay ..  together Speaking about her relationship with Prince in an interview with Norwegian station NRK in November 2014 Sinead sinéad, Said i did meet him a couple of. Times we'didn t get on at. All in, fact we had a-punch.. Up she: Continued he summoned me to his house 'After Nothing compares 2.' U i made it without. him\n",
      "Background [edit]. In 1985, The Family, a funk band created as an outlet to release more of Prince 's music, released their first and only album, the self-titled The Family. Nothing Compares 2 U appeared on the album but it was not released as a single, and received little recognition. Speaking about her relationship with Prince in an interview with Norwegian station NRK in November 2014 Sinead sinéad, Said i did meet him a couple of. Times we'didn t get on at. All in, fact we had a-punch.. Up she: Continued he summoned me to his house 'After Nothing compares 2.' U i made it without. him\n",
      "Sinead Sinéad Marie Bernadette’O (/connor ʃɪˈneɪd/ ; oʊˈkɒnər born 8 december) 1966 is An irish-singer songwriter who rose to fame in the late 1980s with her debut Album The lion and The. Cobra’O connor achieved worldwide success in 1990 with a new arrangement Of ’prince s  Song Nothing compares 2 . u O'Connor was born in Glenageary in County Dublin and was named after Sinead sinéad De, valera wife Of Irish Éamon eamon éamon de valera and mother of the doctor presiding over, the Delivery And saint Bernadette. Of lourdes she is the third of, five children sister To, Novelist, Joseph, eimear John. and eoin\n",
      "Chris Hill, the co-director of O'Connor's label Ensign, recalled to Mojo magazine January 2009 the first time he heard this song: Fachtna O'Kelly, Sinead'sinéad s, manager brought in a cassette and When i heard It i actually started. crying - Their tearful songs were their first #1 hits, but both were written by others. O'Connor's song was written by Prince, Cyrus' by a team of five professional writer/producers. Aretha Franklin covered this for her 2014 album, Aretha Franklin Sings the Great Diva Classics.\n",
      "+8 other releases. read more. Nothing Compares 2 U is the sixth track on The Family's first and only album The Family. Prince wrote seven of the eight songs on the album, but Nothing Compares 2 U is the only track for which Prince takes official credit. Sinead's version is the cover. I believe that the studio version of this song is on Prince's 1st album, titled Prince (early 80s). Check it out! By the way, I love both versions-both artists are amazing and bring their own genius to the song.\n",
      "Original 80's pop video of Sinead O'Connor interpreting her song Nothing Compares 2 You. Nothing Compares 2 U is a song written by Prince for his side-project The Family. \n"
     ]
    }
   ],
   "source": [
    "d = dataset['validation']\n",
    "i = 200\n",
    "negatives = [t for idx, t in enumerate(d[i]['passages']['passage_text']) if d[i]['passages']['is_selected'][idx]==0]\n",
    "positives = [t for idx, t in enumerate(d[i]['passages']['passage_text']) if d[i]['passages']['is_selected'][idx]==1]\n",
    "print(\"Question:\",d[i]['query'])\n",
    "print()\n",
    "print(\"True Passage\")\n",
    "for p in positives:\n",
    "    print(p)\n",
    "print()\n",
    "print(\"False Passage\")\n",
    "for idx, n in enumerate(negatives):\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4560bf1112874376b32f46b6e6f087ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bc899f0b2344eeab5a02500c46f6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ed80e519f84098b469307a6763a977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/187k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5567fabc403843049bb833b426211181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ef35dad7f342e382df63a016b38dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/707 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d01d88d6302493d9981869bec10d098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/35.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97b610e267b411eb3ab370b902cd38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/40181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "qap = load_dataset('enelpol/rag-mini-bioasq', 'question-answer-passages')\n",
    "corpus = load_dataset('enelpol/rag-mini-bioasq', 'text-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2460fb7ea36749a18ac33041065d8978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2102860"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "q2 = qap['train'].rename_column('relevant_passage_ids','gold_titles')\n",
    "q3 = qap['test'].rename_column('relevant_passage_ids','gold_titles')\n",
    "q= concatenate_datasets([q2,q3])\n",
    "q.to_json('data/mini-bioasq_query.jsonl',orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2d9cfaa6684c4d989b9fd40cdf198b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "61976973"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = corpus['test'].rename_column('id','title')\n",
    "c2.to_json('data/mini-bioasq_t2p.jsonl', orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkdrnjs0621/miniconda3/envs/torch_241/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1546167/1954674706.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vec_passages=torch.load(vector)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c09cf0b17ea4355ad4668a685030799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54957])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(vec_passages, vector)\n\u001b[1;32m     70\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m, data_files\u001b[38;5;241m=\u001b[39mquery)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 71\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_nearest\u001b[49m\u001b[43m,\u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvec_passages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m dataset\u001b[38;5;241m.\u001b[39mto_json(result, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_241/lib/python3.9/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_241/lib/python3.9/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_241/lib/python3.9/site-packages/datasets/arrow_dataset.py:3167\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3163\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3164\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3165\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3166\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3168\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3169\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_241/lib/python3.9/site-packages/datasets/arrow_dataset.py:3528\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3526\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3527\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3528\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3530\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_241/lib/python3.9/site-packages/datasets/arrow_dataset.py:3427\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3426\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3427\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3429\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3430\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3431\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m, in \u001b[0;36mmap_nearest\u001b[0;34m(row, space, model, tokenizer)\u001b[0m\n\u001b[1;32m     53\u001b[0m l \u001b[38;5;241m=\u001b[39m get_similarities(space,row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m],model,tokenizer)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(l\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m()\n\u001b[1;32m     56\u001b[0m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m l\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import argparse\n",
    "import re\n",
    "from functools import partial\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "t2p='data/2wikimultihopqa_dev_t2p.jsonl'\n",
    "vector='data/2wikimultihopqa.pt'\n",
    "query='data/2wikimultihopqa_dev_query_1000.jsonl'\n",
    "result='data/2wikimultihopqa_dev_query_1000_scored.jsonl'\n",
    "\n",
    "def mean_pooling(token_embeddings, mask):\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings\n",
    "\n",
    "def encode_batch(data, tokenizer, model, batch_size=128):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(data), batch_size)):\n",
    "            batch = data[i:i + batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = mean_pooling(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "            embeddings.append(batch_embeddings.cpu())\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "encoder_model = 'facebook/contriever'\n",
    "\n",
    "def get_similarities(search_space, query, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "        query_embedding = mean_pooling(model(**query_inputs).last_hidden_state,query_inputs['attention_mask']).cpu()  # Move to CPU\n",
    "        query_embedding = query_embedding / query_embedding.norm(dim=1, keepdim=True)\n",
    "        search_space = search_space / search_space.norm(dim=1, keepdim=True)\n",
    "        similarities = torch.matmul(search_space, query_embedding.T).squeeze()\n",
    "        return similarities\n",
    "    \n",
    "\n",
    "def map_nearest(row,space,model,tokenizer):\n",
    "    l = get_similarities(space,row['question'],model,tokenizer)\n",
    "    row[\"score\"] = l\n",
    "    return row\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_model)\n",
    "model = AutoModel.from_pretrained(encoder_model).to('cuda')\n",
    "\n",
    "if os.path.exists(vector):\n",
    "    vec_passages=torch.load(vector)\n",
    "else:\n",
    "    with open(t2p, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line)['passage'] for line in file]\n",
    "    vec_passages = encode_batch(data, tokenizer, model)\n",
    "    torch.save(vec_passages, vector)\n",
    "\n",
    "dataset = load_dataset('json', data_files=query)[\"train\"]\n",
    "dataset = dataset.map(partial(map_nearest,space=vec_passages,model=model,tokenizer=tokenizer,))\n",
    "dataset.to_json(result, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import argparse\n",
    "import re\n",
    "from functools import partial\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset('json', data_files=query)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 12674, 12695,  1074,    11,  1287,  1422,   479,     2]]), 'entity_ids': tensor([[1657,   32]]), 'entity_position_ids': tensor([[[ 1,  2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [ 5,  6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'entity_attention_mask': tensor([[1, 1]])}\n",
      "<s> Beyoncé lives in Los Angeles.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-tacred were not used when initializing LukeForEntityPairClassification: ['luke.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: per:cities_of_residence\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "from transformers import LukeTokenizer, LukeModel, LukeForEntityPairClassification\n",
    "\n",
    "model = LukeModel.from_pretrained(\"studio-ousia/luke-base\")\n",
    "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n",
    "\n",
    "text = \"Beyoncé lives in Los Angeles.\"\n",
    "\n",
    "entities = [\n",
    "    \"Beyoncé\",\n",
    "    \"Los Angeles\",\n",
    "]  # Wikipedia entity titles corresponding to the entity mentions \"Beyoncé\" and \"Los Angeles\"\n",
    "entity_spans = [(0, 7), (17, 28)]  # character-based entity spans corresponding to \"Beyoncé\" and \"Los Angeles\"\n",
    "inputs = tokenizer(text, entities=entities, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(tokenizer.decode(token_ids=inputs['input_ids'][0]))\n",
    "outputs = model(**inputs)\n",
    "word_last_hidden_state = outputs.last_hidden_state\n",
    "entity_last_hidden_state = outputs.entity_last_hidden_state\n",
    "\n",
    "model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n",
    "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n",
    "entity_spans = [(0, 7), (17, 28)]  # character-based entity spans corresponding to \"Beyoncé\" and \"Los Angeles\"\n",
    "inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class_idx = int(logits[0].argmax())\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkdrnjs0621/miniconda3/envs/torch_241/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of LukeForTokenClassification were not initialized from the model checkpoint at studio-ousia/luke-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LukeForTokenClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n",
    "model = LukeForTokenClassification.from_pretrained(\"studio-ousia/luke-base\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkdrnjs0621/miniconda3/envs/torch_241/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of LukeForTokenClassification were not initialized from the model checkpoint at studio-ousia/luke-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '</s>', '<unk>', '<pad>', '<mask>', '<ent>', '<ent2>', '<ent>', '<ent2>']\n",
      "Predicted word: <pad>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LukeForTokenClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n",
    "model = LukeForTokenClassification.from_pretrained(\"studio-ousia/luke-base\")\n",
    "\n",
    "# Input text with [MASK] token\n",
    "text = \"<s>The capital of France is Paris.</s> France <mask>\"\n",
    "# print(tokenizer.decode(tokenizer.separation_token_id))\n",
    "# Tokenize input\n",
    "print(tokenizer.all_special_tokens)\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "masked_index = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Decode predicted token\n",
    "predicted_token_id = predictions[0, masked_index].argmax(dim=-1).item()\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "\n",
    "print(f\"Predicted word: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 100.0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path = 'data/entityrag/2wikimultihopqa_result_or2f.jsonl'\n",
    "dataset = load_dataset('json', data_files=dataset_path)[\"train\"]\n",
    "a = 0 \n",
    "for row in dataset:\n",
    "    for gt in row['gold_titles']:\n",
    "        gnt = gt\n",
    "        if('(' in gnt):\n",
    "            gnt = gnt.split('(')[0]\n",
    "        gnt = gnt.strip().lower()\n",
    "        if gnt in row['question'].lower():\n",
    "            a+=1\n",
    "            break\n",
    "print(a, len(dataset), a/len(dataset)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_241",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
